---
title: 本地部署DeepSeek R1 70B 记录
date: 2025-02-15 17:03:05
tags: LLM
---

## 写在前面

以前感觉这种本地部署很麻烦，遇到问题需要搜索半天，当然对于一个极客来说，也是乐趣所在。不过这次虽然稍微走了一下弯路，但总体还算顺利。为什么要本地部署DeepSeek呢？首先是国内新闻的大幅度报道，作为国产模型的顶流，作为大模型发展的前言让我很想试试，把玩一下。其次现在DeepSeek APP限流很严重，很多时候无法使用，毫无使用体验可言。最重要的是，我司之前提供了一台GPU服务器，一直在空闲。所以，我决定尝试一下本地部署DeepSeek R1 70B。

## 服务器配置
| 组件 | 型号 | 容量 |
|------|------|------|
| CPU | AMD Ryzen Threadripper 3960X | 24核 48线程 |
| 内存 | Samsung 3200MT 32GB*8 | 256GB |
| GPU | NVIDIA RTX A6000 48GB*2 | 96GB |
| 硬盘 | PNY CS3030 2TB SSD | 2TB |
| 硬盘 | Micron 5210 MTFD | 2TB |
| 网络 | RTL8125 | 2.5GbE |
| 系统 | Ubuntu 20.04.3 LTS |  |

不得不感叹一下，作为一台debug server， 这配置绰绰有余了。


## 部署步骤
### 1. 安装Docker
之前安装1panel面板的时候已经安装好了，因此这里就不再赘述。

### 2. 准备ollama docker
参考dockerhub ollama 部署docker的[官方文档](https://hub.docker.com/r/ollama/ollama)。

Step1: Configure the repository
```shell
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
```

Step2: Install the NVIDIA Container Toolkit packages
```shell
sudo apt-get install -y nvidia-container-toolkit
```

Step3: Configure Docker to use Nvidia driver
```shell
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

Step4: Start the ollama container
```shell
docker run -d \
  --gpus '"device=0"' \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  -e OLLAMA_HOST=0.0.0.0 \
  -e OLLAMA_ORIGINS=* \
  -e CUDA_VISIBLE_DEVICES=0 \
  --name ollama \
  ollama/ollama
```
这里我只分配了gpu0 做 deepseek r1 的推理。

### 3. 安装Page Assist
Page Assist 是一款开源的浏览器扩展程序，主要为本地 AI 模型提供直观的交互界面，让用户可以在任何网页上与本地 AI 模型进行对话和交互。
[插件链接](https://chromewebstore.google.com/detail/page-assist-%E6%9C%AC%E5%9C%B0-ai-%E6%A8%A1%E5%9E%8B%E7%9A%84-web/jfgfiigpkhlkbnfnbobbkinehhfdhndo)
现在只上架了chrome商店，我直接使用Edge打开商店进行了安装。


### 4. 部署DeepSeek R1 70B
Step1: 打开Page Assist插件，点击右上角的设置按钮，选择ollama设置,输入ollama的地址，点击保存。
![alt text](/images/2025-02-151.png)

Step2：打开管理模型，点击添加新模型，输入模型名称：`deepseek-r1:70b`

ollama 支持的模型可以在这里查看：[ollama模型列表](https://ollama.com/search)
为什么选择DeepSeek R1 70B呢？70B大概占用44GB的显存，这个配置刚好可以支持。
![alt text](/images/2025-02-153.png)

### 5. 打开联网功能
不能联网的LLM都有很大的局限,我选择的是Google。
![alt text](/images/2025-02-152.png)



### 6. 测试
可以愉快的玩耍了。
![alt text](/images/2025-02-154.png)

显存和显卡利用率：
![alt text](/images/2025-02-155.png)


### 7. 踩过的坑
1. 最开始想用 ollama+open webui 的思路，使用
cursor直接生成docker file，但是发现cursor 生成的webui 的下载链接一直没搞对，遂放弃。后来看webui官网文档，安装命令一键搞定。

2. 在部署完成后发现 webui 的联网功能仅支持dockdockgo，剩下的需要填写api key。 申请key的时候，Google 需要外币信用卡，遂放弃。Bing的key申请一直不成功，也放弃。最后切换到了page assist插件。
